{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3062c605-0dd1-44ac-bac6-f0163c652156",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# === Load raw data ===\u001b[39;00m\n\u001b[1;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFood_Inspections_20250216.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === Load raw data ===\n",
    "df = pd.read_csv(\"Food_Inspections_20250216.csv\")\n",
    "\n",
    "# === Drop unstructured or irrelevant columns (optional but recommended) ===\n",
    "df = df.drop(columns=[\"Violations\", \"Latitude\", \"Longitude\", \"Location\"], errors=\"ignore\")\n",
    "\n",
    "# === Drop rows with missing values in essential fields ===\n",
    "df = df.dropna()\n",
    "\n",
    "# === Convert float columns to string for categorical matching (FDs/INDs prefer discrete values) ===\n",
    "float_cols = df.select_dtypes(include=[\"float64\"]).columns\n",
    "df[float_cols] = df[float_cols].astype(int).astype(str)\n",
    "\n",
    "# === Normalize all object columns (trim and lowercase for IND checking) ===\n",
    "for col in df.select_dtypes(include=[\"object\"]).columns:\n",
    "    df[col] = df[col].str.strip().str.lower()\n",
    "\n",
    "# === Export cleaned version for FD/IND analysis ===\n",
    "df.to_csv(\"Food_Inspections_Cleaned.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadae52c-7e1b-4a06-9813-ead5cc59960c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from tane import TANE, read_db\n",
    "\n",
    "# === Step 1: Load cleaned dataset and prepare sample ===\n",
    "df = pd.read_csv(\"Food_Inspections_Cleaned.csv\")\n",
    "columns_to_use = [\n",
    "    \"Inspection ID\", \"License #\", \"DBA Name\", \"Facility Type\",\n",
    "    \"Risk\", \"City\", \"State\", \"Results\"\n",
    "]\n",
    "df_sample = df[columns_to_use].head(2000)\n",
    "sample_csv = \"Food_Inspections_Sample.csv\"\n",
    "df_sample.to_csv(sample_csv, index=False)\n",
    "\n",
    "# === Step 2: Load actual column names from saved CSV ===\n",
    "column_names = pd.read_csv(sample_csv, nrows=0).columns.tolist()\n",
    "\n",
    "# === Step 3: Run TANE ===\n",
    "T = read_db(sample_csv)\n",
    "tane = TANE(T)\n",
    "\n",
    "start = time.time()\n",
    "tane.run()\n",
    "end = time.time()\n",
    "\n",
    "# === Step 4: Print results safely ===\n",
    "print(f\"\\n Execution Time: {end - start:.2f} seconds\")\n",
    "print(f\" {len(tane.rules)} FDs found:\\n\")\n",
    "\n",
    "for lhs, rhs in tane.rules:\n",
    "    if rhs < len(column_names) and all(i < len(column_names) for i in lhs):\n",
    "        lhs_names = [column_names[i] for i in lhs]\n",
    "        rhs_name = column_names[rhs]\n",
    "        print(f\"{', '.join(lhs_names)} → {rhs_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680c27fa-9d2e-4353-9666-fd00699b5941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from itertools import combinations\n",
    "\n",
    "def auto_fd_search(df, max_lhs=2):\n",
    "    results = []\n",
    "    start = time.time()\n",
    "    columns = df.columns.tolist()\n",
    "\n",
    "    for rhs in columns:\n",
    "        lhs_candidates = [col for col in columns if col != rhs]\n",
    "        for lhs in combinations(lhs_candidates, max_lhs):\n",
    "            grouped = df.groupby(list(lhs))[rhs].nunique()\n",
    "            if grouped.max() == 1:\n",
    "                results.append((list(lhs), rhs))\n",
    "\n",
    "    end = time.time()\n",
    "    print(f\"\\n max_lhs = {max_lhs} |  Runtime: {end - start:.2f} seconds\")\n",
    "    print(f\" {len(results)} functional dependencies found:\\n\")\n",
    "\n",
    "    for lhs, rhs in results:\n",
    "        print(f\"{', '.join(lhs)} → {rhs}\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b0de26-aed9-4e58-9b6e-25a5756f88d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Food_Inspections_Sample.csv\")\n",
    "\n",
    "# 1 → 1 FDs\n",
    "auto_fd_search(df, max_lhs=1)\n",
    "\n",
    "# 2 → 1 FDs\n",
    "auto_fd_search(df, max_lhs=2)\n",
    "\n",
    "# 3 → 1 (optional but slower)\n",
    "auto_fd_search(df, max_lhs=3)\n",
    "\n",
    "# 4 → 1 (optional but slower)\n",
    "auto_fd_search(df, max_lhs=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a709af-3dbc-4f70-bc5e-0e54ff0aff69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from itertools import combinations\n",
    "\n",
    "# Step 1: Brute-force FD discovery\n",
    "def auto_fd_search(df, max_lhs=2):\n",
    "    results = []\n",
    "    columns = df.columns.tolist()\n",
    "    for rhs in columns:\n",
    "        lhs_candidates = [col for col in columns if col != rhs]\n",
    "        for lhs in combinations(lhs_candidates, max_lhs):\n",
    "            grouped = df.groupby(list(lhs))[rhs].nunique()\n",
    "            if grouped.max() == 1:\n",
    "                results.append((list(lhs), rhs))\n",
    "    return results\n",
    "\n",
    "# Step 2: Filter for only minimal FDs\n",
    "def filter_minimal_fds(fds):\n",
    "    minimal = []\n",
    "    for lhs, rhs in fds:\n",
    "        is_minimal = True\n",
    "        for other_lhs, other_rhs in fds:\n",
    "            if rhs == other_rhs and set(other_lhs).issubset(set(lhs)) and lhs != other_lhs:\n",
    "                is_minimal = False\n",
    "                break\n",
    "        if is_minimal:\n",
    "            minimal.append((lhs, rhs))\n",
    "    return minimal\n",
    "\n",
    "# Step 3: Run across multiple LHS sizes and combine\n",
    "def discover_minimal_fds(df, max_lhs=4):\n",
    "    combined_results = []\n",
    "    for i in range(1, max_lhs + 1):\n",
    "        print(f\" Searching FDs with max_lhs = {i} ...\")\n",
    "        result = auto_fd_search(df, max_lhs=i)\n",
    "        combined_results.extend(result)\n",
    "    minimal = filter_minimal_fds(combined_results)\n",
    "\n",
    "    print(f\"\\n Final set of minimal FDs (after pruning redundant ones): {len(minimal)}\\n\")\n",
    "    for lhs, rhs in minimal:\n",
    "        print(f\"{', '.join(lhs)} → {rhs}\")\n",
    "    return minimal\n",
    "\n",
    "df = pd.read_csv(\"Food_Inspections_Sample.csv\")\n",
    "start_time = time.time()\n",
    "minimal_fds = discover_minimal_fds(df, max_lhs=5)\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n Total Runtime (all steps): {total_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d639491-1801-4a93-914b-a18a986ba957",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"Food_Inspections_Cleaned.csv\")\n",
    "\n",
    "# Function to find unary INDs using a value index\n",
    "def find_unary_inds_via_index(df):\n",
    "    value_index = {}\n",
    "\n",
    "    # Step 1: Build value index (value -> set of columns where it appears)\n",
    "    for col in df.columns:\n",
    "        for val in df[col].dropna().unique():\n",
    "            if val not in value_index:\n",
    "                value_index[val] = set()\n",
    "            value_index[val].add(col)\n",
    "\n",
    "    # Step 2: Check for unary INDs\n",
    "    unary_inds = []\n",
    "    for lhs in df.columns:\n",
    "        lhs_values = df[lhs].dropna().unique()\n",
    "        for rhs in df.columns:\n",
    "            if lhs == rhs:\n",
    "                continue\n",
    "            # Check if every value in lhs also appears in rhs\n",
    "            if all(rhs in value_index.get(val, set()) for val in lhs_values):\n",
    "                unary_inds.append((lhs, rhs))\n",
    "    return unary_inds\n",
    "\n",
    "# Run the discovery and print results\n",
    "inds = find_unary_inds_via_index(df)\n",
    "print(\"Unary INDs found (LHS ⊆ RHS):\")\n",
    "for lhs, rhs in inds:\n",
    "    print(f\"{lhs} ⊆ {rhs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24556f9a-cdd6-4d09-ab67-591980f900ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70c0990-3f31-4e28-a6bf-d02b35d4fe46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1430a18b-7874-4e94-8074-04e8c0dd41c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
